{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import torch \n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from huggingface_hub import login\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('configure.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be312368",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.memory._set_allocator_settings(\"expandable_segments:True\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b703a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA available. GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"CUDA not available. Using CPU.\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbdea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('json', data_files=r\"your_new_dataset_path.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f17bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = config.get('model', 'base_model')\n",
    "# previous_fine_tuned_model = config.get('model', 'resume_model')\n",
    "\n",
    "model_id = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "resume_model_path = r\"D:\\Camtour\\src\\model\\chatbot_v0.2\\checkpoint-750\"\n",
    "output_dir = r\"D:\\Camtour\\src\\model\\chatbot_v0.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2552385",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(resume_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d254f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_format(examples):\n",
    "    try:\n",
    "        prompt = tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)\n",
    "        return {\"prompt\": prompt}\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing messages: {e}\")\n",
    "        return {\"prompt\": \"\"}\n",
    "\n",
    "format_dataset = dataset.map(chat_format, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "print(\"Sample formatted prompts:\")\n",
    "for i in range(min(2, len(format_dataset['train']))):\n",
    "    print(f\"Example {i}: {format_dataset['train'][i]['prompt'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"prompt\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=2048,  \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "tokenized_dataset = format_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    batch_size=8,\n",
    "    remove_columns=[\"prompt\"] \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e02107",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading previously fine-tuned model...\")\n",
    "previous_fine_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    resume_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lora_config = LoraConfig(\n",
    "    r=32,  \n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "print(\"Applying new LoRA configuration...\")\n",
    "model = get_peft_model(previous_fine_tuned_model, new_lora_config)\n",
    "\n",
    "print(f\"Model trainable parameters: {model.num_parameters()/1e6:.2f}M\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d2395",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4, \n",
    "    fp16=True,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_steps=50,\n",
    "    report_to=None,\n",
    "    disable_tqdm=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    save_safetensors=True,\n",
    "    logging_first_step=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset.get(\"test\"),\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
