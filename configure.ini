[paths]
data_dir = "YOUR_DATA_PATH"
output_dir = "YOUR_OUTPUT_PATH"

[training]
batch_size = 8
learning_rate = 1e-5
num_epochs = 3
gradient_accumulation_steps = 4
lora_r = 16 # Rank for LoRA can be 4, 8, 16, 32, 64 based on model size
lora_alpha = 32
lora_dropout = 0.1
use_flash_attention = true
max_seq_length = 2048

[model]
base_model = "mistralai/Mistral-7B-Instruct-v0.3"
resume_model = "YOUR_RESUME_MODEL"